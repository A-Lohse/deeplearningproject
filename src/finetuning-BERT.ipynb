{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler, DataCollatorWithPadding\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_rename_columns(path:str, rename_dict={}, drop_columns_dict={}, drop_congress_number=[])->pd.DataFrame:\n",
    "    #load data\n",
    "    df = pd.read_pickle(path)\n",
    "    df = df.loc[~df['cong'].isin(drop_congress_number)].copy()\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    df = df.drop(columns=drop_columns_dict)\n",
    "    return df\n",
    "def clear():\n",
    "    os.system( 'cls' )\n",
    "    \n",
    "def train_validation_split(df, train_frac,eval_frac,random_seed):\n",
    "    df_train = df.sample(frac=train_frac, random_state=random_seed)\n",
    "    df_eval = df.sample(frac=eval_frac, random_state=random_seed)\n",
    "    return df_train, df_eval\n",
    "\n",
    "def create_dataset_object_from_pandas_dataframe(df,columns_to_be_removed):\n",
    "    dataset = datasets.Dataset.from_pandas(df).remove_columns(columns_to_be_removed)\n",
    "    return dataset\n",
    "\n",
    "def tokenizer_function(bill):\n",
    "    return tokenizer(bill[\"sentences\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    class_sample_count = np.array([len(np.where(dataset[\"labels\"] == t)[0]) for t in np.unique(dataset[\"labels\"])])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in dataset[\"labels\"]])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weight = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, loss_function, optimizer, num_epochs, lr_scheduler_function, device):\n",
    "    \n",
    "\n",
    "    model.train().to(device)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    lr_scheduler = get_scheduler(\n",
    "        lr_scheduler_function,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_training_steps/10,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        train_targs, train_preds = [], []\n",
    "        val_targs, val_preds = [], []\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_function(outputs.logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)            \n",
    "\n",
    "            #Getting metrics\n",
    "            train_targs += list(batch[\"labels\"].cpu().numpy())\n",
    "            train_preds += list(predictions.cpu().numpy())\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        \n",
    "        print('-----------Training Metrics-----------')\n",
    "        print('Accuracy: {}'.format(accuracy_score(train_targs, train_preds)))\n",
    "        print('F1: {}'.format(f1_score(train_targs, train_preds)))\n",
    "        print('Precision: {}'.format(precision_score(train_targs, train_preds)))\n",
    "        print('Recall: {}'.format(recall_score(train_targs, train_preds)))\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion_matrix(train_targs, train_preds))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():   \n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)            \n",
    "\n",
    "                #Getting metrics\n",
    "                val_targs += list(batch[\"labels\"].cpu().numpy())\n",
    "                val_preds += list(predictions.cpu().numpy())\n",
    "\n",
    "        print('-----------Validation Metrics-----------')\n",
    "        print('Accuracy: {}'.format(accuracy_score(val_targs, val_preds)))\n",
    "        print('F1: {}'.format(f1_score(val_targs, val_preds)))\n",
    "        print('Precision: {}'.format(precision_score(val_targs, val_preds)))\n",
    "        print('Recall: {}'.format(recall_score(val_targs, val_preds)))\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion_matrix(val_targs, val_preds))\n",
    "        print('-' * 66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Combining list of sentences with [SEP] tokens...\n",
      "Training validation split...\n",
      "Converting pandas df to dataset-object...\n",
      "Loading tokenizer...\n",
      "Applying tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:14<00:00,  1.25ba/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.83ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloader with batches using a weighted sampler\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2948/11792 [1:08:44<1:18:12,  1.88it/s, loss=0.00281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training Metrics-----------\n",
      "Accuracy: 0.8368950701040253\n",
      "F1: 0.8408978106215188\n",
      "Precision: 0.8324052844196965\n",
      "Recall: 0.8495654111878761\n",
      "Confusion Matrix:\n",
      "[[7179 1535]\n",
      " [1350 7624]]\n",
      "-----------Validation Metrics-----------\n",
      "Accuracy: 0.9647218453188603\n",
      "F1: 0.6623376623376623\n",
      "Precision: 0.504950495049505\n",
      "Recall: 0.9622641509433962\n",
      "Confusion Matrix:\n",
      "[[4113  150]\n",
      " [   6  153]]\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5896/11792 [2:33:39<50:06,  1.96it/s, loss=0.00334]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training Metrics-----------\n",
      "Accuracy: 0.9803821800090456\n",
      "F1: 0.9802019740971073\n",
      "Precision: 0.9768023652490334\n",
      "Recall: 0.9836253292110386\n",
      "Confusion Matrix:\n",
      "[[8751  204]\n",
      " [ 143 8590]]\n",
      "-----------Validation Metrics-----------\n",
      "Accuracy: 0.9771596562641339\n",
      "F1: 0.7577937649880095\n",
      "Precision: 0.6124031007751938\n",
      "Recall: 0.9937106918238994\n",
      "Confusion Matrix:\n",
      "[[4163  100]\n",
      " [   1  158]]\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 8844/11792 [3:58:36<1:31:30,  1.86s/it, loss=0.00107]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training Metrics-----------\n",
      "Accuracy: 0.99400723654455\n",
      "F1: 0.9939380075488963\n",
      "Precision: 0.992235670244348\n",
      "Recall: 0.9956461961503208\n",
      "Confusion Matrix:\n",
      "[[8892   68]\n",
      " [  38 8690]]\n",
      "-----------Validation Metrics-----------\n",
      "Accuracy: 0.9972862957937585\n",
      "F1: 0.9634146341463414\n",
      "Precision: 0.9349112426035503\n",
      "Recall: 0.9937106918238994\n",
      "Confusion Matrix:\n",
      "[[4252   11]\n",
      " [   1  158]]\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11792/11792 [5:23:57<00:00,  1.45s/it, loss=0.000188]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training Metrics-----------\n",
      "Accuracy: 0.9983604703753958\n",
      "F1: 0.998376714245732\n",
      "Precision: 0.9978740069374511\n",
      "Recall: 0.9988799283154122\n",
      "Confusion Matrix:\n",
      "[[8741   19]\n",
      " [  10 8918]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11792/11792 [5:30:44<00:00,  1.68s/it, loss=0.000188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Validation Metrics-----------\n",
      "Accuracy: 0.998869289914066\n",
      "F1: 0.9845201238390092\n",
      "Precision: 0.9695121951219512\n",
      "Recall: 1.0\n",
      "Confusion Matrix:\n",
      "[[4258    5]\n",
      " [   0  159]]\n",
      "------------------------------------------------------------------\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df = load_data_and_rename_columns('data/processed/bert_data.pickle',\n",
    "            rename_dict={\"status\":\"labels\"},\n",
    "            drop_columns_dict={'bill_id','cong'},\n",
    "            drop_congress_number=[])\n",
    "\n",
    "print(\"Combining list of sentences with [SEP] tokens...\")\n",
    "df['sentences'] = df['sentences'].apply(lambda x: '[SEP] '.join(x))\n",
    "\n",
    "print(\"Training validation split...\")\n",
    "df_train, df_eval = train_validation_split(df, 0.8, 0.2, random_seed=3060)\n",
    "# class_weights = list(float(x) for x in compute_class_weight('balanced', classes=df_train[\"labels\"].unique(), y=df_train[\"labels\"]))\n",
    "\n",
    "print(\"Converting pandas df to dataset-object...\")\n",
    "dataset_train = create_dataset_object_from_pandas_dataframe(df_train, \"__index_level_0__\")\n",
    "dataset_eval = create_dataset_object_from_pandas_dataframe(df_eval, \"__index_level_0__\")\n",
    "dataset = datasets.DatasetDict({\"train\":dataset_train,\"eval\":dataset_eval})\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Applying tokenizer\")\n",
    "dataset_tokenized = dataset.map(tokenizer_function, batched=True)\n",
    "dataset_tokenized[\"train\"] = dataset_tokenized[\"train\"].remove_columns(\"sentences\")\n",
    "dataset_tokenized[\"eval\"] = dataset_tokenized[\"eval\"].remove_columns(\"sentences\")\n",
    "dataset_tokenized.set_format(\"torch\") #converting lists to tensors\n",
    "\n",
    "print(\"Creating dataloader with batches using a weighted sampler\")\n",
    "sampler = create_weighted_sampler(dataset_tokenized[\"train\"])\n",
    "train_dataloader = DataLoader(dataset_tokenized[\"train\"], batch_size=6, drop_last=True, sampler=sampler)\n",
    "eval_dataloader = DataLoader(dataset_tokenized[\"eval\"], batch_size=6, drop_last = True)\n",
    "\n",
    "print(\"Loading model\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "print(\"Training model\")\n",
    "#Arguments:\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 4\n",
    "lr_scheduler_function = \"linear\"\n",
    "# class_weights = torch.tensor(device=device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Training function\n",
    "train(model, train_dataloader, eval_dataloader,loss_function, optimizer, num_epochs, lr_scheduler_function, device)\n",
    "\n",
    "print(\"Saving model\")\n",
    "torch.save(model, \"results/BERT_finetuned_congress_103_115_4_epochs_80pct_train_20_val.pt\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[This Act may be cited as the \"Public Housing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[This Act may be cited as the \"Targeted Econom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The Administrator shall conduct a study of St...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Medicaid Benefits Continued for 36 Months for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[This Act may be cited as the \"Expedited Consi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22110</th>\n",
       "      <td>[In this section the term \"Administrative Proc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22111</th>\n",
       "      <td>[Section 222 of the Communications Act of 1934...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22112</th>\n",
       "      <td>[This Act may be cited as the \"Safe Drinking W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22113</th>\n",
       "      <td>[This Act may be cited as the \"Preserving Data...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22114</th>\n",
       "      <td>[This Act may be cited as the \"Helping to Enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  labels\n",
       "0      [This Act may be cited as the \"Public Housing ...       0\n",
       "1      [This Act may be cited as the \"Targeted Econom...       0\n",
       "2      [The Administrator shall conduct a study of St...       0\n",
       "3      [Medicaid Benefits Continued for 36 Months for...       0\n",
       "4      [This Act may be cited as the \"Expedited Consi...       0\n",
       "...                                                  ...     ...\n",
       "22110  [In this section the term \"Administrative Proc...       0\n",
       "22111  [Section 222 of the Communications Act of 1934...       0\n",
       "22112  [This Act may be cited as the \"Safe Drinking W...       0\n",
       "22113  [This Act may be cited as the \"Preserving Data...       0\n",
       "22114  [This Act may be cited as the \"Helping to Enco...       0\n",
       "\n",
       "[22115 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data_and_rename_columns('data/processed/bert_data.pickle',\n",
    "            rename_dict={\"status\":\"labels\"},\n",
    "            drop_columns_dict={'bill_id','cong'},\n",
    "            drop_congress_number=[])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Combining list of sentences with [SEP] tokens...\n",
      "Converting pandas df to dataset-object...\n",
      "Loading tokenizer...\n",
      "Applying tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:22<00:00,  1.03ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloader\n",
      "Calculating predictions of the voting behaviour of the 115th congress\n",
      "-----------Test Metrics-----------\n",
      "Accuracy: 0.9918136589778381\n",
      "F1: 0.8747404844290658\n",
      "Precision: 0.9362962962962963\n",
      "Recall: 0.8207792207792208\n",
      "Confusion Matrix:\n",
      "[[21297    43]\n",
      " [  138   632]]\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df = load_data_and_rename_columns('data/processed/bert_data.pickle',\n",
    "            rename_dict={\"status\":\"labels\"},\n",
    "            drop_columns_dict={'bill_id','cong'})\n",
    "\n",
    "print(\"Combining list of sentences with [SEP] tokens...\")\n",
    "df['sentences'] = df['sentences'].apply(lambda x: '[SEP] '.join(x))\n",
    "\n",
    "print(\"Converting pandas df to dataset-object...\")\n",
    "dataset_test = create_dataset_object_from_pandas_dataframe(df, \"__index_level_0__\")\n",
    "dataset_test = datasets.DatasetDict({\"test\":dataset_test})\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Applying tokenizer\")\n",
    "dataset_test_tokenized = dataset_test.map(tokenizer_function, batched=True)\n",
    "dataset_test_tokenized[\"test\"] = dataset_test_tokenized[\"test\"].remove_columns(\"sentences\")\n",
    "dataset_test_tokenized.set_format(\"torch\") #converting lists to tensors\n",
    "\n",
    "print(\"Creating dataloader\")\n",
    "test_dataloader = DataLoader(dataset_test_tokenized[\"test\"], batch_size=6, drop_last=True)\n",
    "\n",
    "test_targs, test_preds = [], []\n",
    "\n",
    "\n",
    "print(\"Calculating predictions of the voting behaviour of the 115th congress\")\n",
    "with torch.no_grad():   \n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)            \n",
    "\n",
    "        #Getting metrics\n",
    "        test_targs += list(batch[\"labels\"].cpu().numpy())\n",
    "        test_preds += list(predictions.cpu().numpy())\n",
    "\n",
    "    print('-----------Test Metrics-----------')\n",
    "    print('Accuracy: {}'.format(accuracy_score(test_targs, test_preds)))\n",
    "    print('F1: {}'.format(f1_score(test_targs, test_preds)))\n",
    "    print('Precision: {}'.format(precision_score(test_targs, test_preds)))\n",
    "    print('Recall: {}'.format(recall_score(test_targs, test_preds)))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(test_targs, test_preds))\n",
    "    print('-' * 66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data_and_rename_columns('data/processed/bert_data.pickle',\n",
    "            rename_dict={\"status\":\"labels\"},\n",
    "            drop_columns_dict={'bill_id'},\n",
    "            drop_congress_number=list(range(100,115)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "      <th>cong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17649</th>\n",
       "      <td>[This Act may be cited as the \"Federal Executi...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17650</th>\n",
       "      <td>[This Act may be cited as the \"CFPB Constituti...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17651</th>\n",
       "      <td>[This Act may be cited as the \"Commute Less Ac...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17652</th>\n",
       "      <td>[This Act may be cited as the \"Zero Waste Deve...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17653</th>\n",
       "      <td>[This Act may be cited as the \"Synthetics Traf...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22110</th>\n",
       "      <td>[In this section the term \"Administrative Proc...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22111</th>\n",
       "      <td>[Section 222 of the Communications Act of 1934...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22112</th>\n",
       "      <td>[This Act may be cited as the \"Safe Drinking W...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22113</th>\n",
       "      <td>[This Act may be cited as the \"Preserving Data...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22114</th>\n",
       "      <td>[This Act may be cited as the \"Helping to Enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  labels  cong\n",
       "17649  [This Act may be cited as the \"Federal Executi...       0   115\n",
       "17650  [This Act may be cited as the \"CFPB Constituti...       0   115\n",
       "17651  [This Act may be cited as the \"Commute Less Ac...       0   115\n",
       "17652  [This Act may be cited as the \"Zero Waste Deve...       0   115\n",
       "17653  [This Act may be cited as the \"Synthetics Traf...       0   115\n",
       "...                                                  ...     ...   ...\n",
       "22110  [In this section the term \"Administrative Proc...       0   115\n",
       "22111  [Section 222 of the Communications Act of 1934...       0   115\n",
       "22112  [This Act may be cited as the \"Safe Drinking W...       0   115\n",
       "22113  [This Act may be cited as the \"Preserving Data...       0   115\n",
       "22114  [This Act may be cited as the \"Helping to Enco...       0   115\n",
       "\n",
       "[1414 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'sentences'],\n",
       "        num_rows: 1414\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Transformer` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29580/3056658305.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# training_args = TrainingArguments(\"test_trainer\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m training_args = TrainingArguments(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./results'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# training_args = TrainingArguments(\"test_trainer\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"eval\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19660/2659390474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_tokenized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning-env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1538\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1541\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning-env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model(torch.tensor(dataset_tokenized[\"train\"][\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2023,  2552,  2089,  2022,  6563,  2004,  1996,  1000, 17581,\n",
       "         3171,  5386,  2552,  1000,  1012,   102,  2000,  2393,  6469,  1998,\n",
       "         2490,  1996, 25954,  2797,  4753,  1999,  2430,  1998,  2789,  2885,\n",
       "         2044,  1996,  2991,  1997,  1996,  4068,  2813,  1010,  3519,  1010,\n",
       "         2083, 26465,  1997,  1996,  2490,  2005,  2264,  2647,  7072,  1006,\n",
       "         6534,  1007,  2552,  1997,  2960,  1998,  1996,  4071,  2490,  2552,\n",
       "         1010,  9362,  3053,  1002,  1015,  1010,  3263,  1010,  2199,  1010,\n",
       "         2199,  2005,  1996,  2142,  2163,  4034,  2005,  2248,  2458,   102,\n",
       "         1996,  3800,  1997,  1996,  9562,  1998, 11453,  1011,  2137,  6960,\n",
       "         4636,  2003,  2000,  5326,  2062,  4235,  4207, 14165,  2083,  2797,\n",
       "         4753,  2458,  1998,  1996,  6043,  1998,  6078,  9530,  8566,  6895,\n",
       "         3726,  2045,  3406,  1999,  9562,  1998, 11453,  1010,  2164,  2083,\n",
       "        10940,  1010, 12702,  4135,  6962,  1010, 10067, 10518,  1010,  5427,\n",
       "         1010, 21586,  1010,  8624,  1010, 24010,  2913,  1010,  4087,  5375,\n",
       "         1010,  3977,  2311,  1997, 15697, 10285,  1998,  2060,  7882,  4411,\n",
       "         1010,  4101, 13252,  1010,  1998,  2060,  5761,  1012,   102,  1996,\n",
       "         9562,  1998, 11453,  1011,  2137,  6960,  4636,  4618,  5326,  2797,\n",
       "         4753,  2458,  2083,  1996, 17890,  1998,  4935,  1997,  6107,  1998,\n",
       "         5618,  8010,  1997,  2797,  9926,  1010,  3391,  2235,  1998,  5396,\n",
       "         1011,  7451,  9926,  1012,   102,  1996, 11643,  1010,  4712,  1010,\n",
       "         1998, 28170,  1997,  2614,  5971, 10615,  1998,  2375,  1011, 11113,\n",
       "        28173,  3070,  2530,  2449,  6078,  1012,   102,  1996,  4712,  1997,\n",
       "         3343,  8818,  2000,  5335,  1996,  2449, 12067,  4044,  1998, 10956,\n",
       "         3097,  1998,  4968,  5211,  1012,   102,  1998,  1996, 10467,  2008,\n",
       "         2797,  4753,  5211,  2064,  2022, 10607,  5618,  8231,  1012,   102,\n",
       "         1996,  2343,  2003,  9362,  2000, 24414,  1037,  2797,  1010, 14495,\n",
       "         3029,  2000,  4374,  5029,  2081,  2800,  2104,  2023,  2552,  2005,\n",
       "         1996,  5682,  9675,  1999,  2930,  1017,  1012,   102,  1996,  9562,\n",
       "         1998, 11453,  1011,  2137,  6960,  4636,  4618,  2022,  9950,  2011,\n",
       "         1037,  2604,  1997,  5501,  1010,  2029,  4618,  2022, 11539,  1997,\n",
       "         1021,  2797,  4480,  1997,  1996,  2142,  2163,  2805,  2011,  1996,\n",
       "         2343,  1997,  1996,  2142,  2163,  1999, 16053,  2007,  1996,  8911,\n",
       "         1997,  1996,  2142,  2163,  4034,  2005,  2248,  2458,  1012,   102,\n",
       "         1996,  2604,  2003,  9362,  2000, 11322,  2039,  2000,  1017,  3176,\n",
       "         2372,  2040,  2024,  4480,  1997,  9562,  1998, 11453,  2065,  3530,\n",
       "         2000, 15645,  2011,  2035,  2372,  1997,  1996,  2604,  1012,   102,\n",
       "         2093,  2372,  1997,  1996,  2604,  1997,  5501,  4618,  2022,  3479,\n",
       "         2013,  2426,  2111,  2007,  3278,  3188,  3325,  1999,  2458,  1998,\n",
       "         2019,  6739,  4824,  1997,  2458, 18402,  2005,  9562,  1998, 11453,\n",
       "         1012,   102,  1996,  2343,  4618, 16823,  1996,  2142,  2163,  6059,\n",
       "         2000,  9562,  1998, 11453,  1010,  2030,  1996,  6059,  1005,  1055,\n",
       "         2640,  4402,  1010,  2004,  2092,  2004,  1996,  3353,  8911,  1997,\n",
       "         1996,  2142,  2163,  4034,  2005,  2248,  2458,  2005,  2885,  1998,\n",
       "         7327, 27839,  1010,  2030,  1996,  3353,  8911,  1005,  1055,  2640,\n",
       "         4402,  1010,  2004, 14975,  2015,  2000,  1996,  2604,  1012,   102,\n",
       "         2045,  2003,  9362,  2000,  2022, 29223,  2005,  1996,  2533,  1997,\n",
       "         2110,  2005, 10807,  2095,  2760,  1002,  2382,  1010,  2199,  1010,\n",
       "         2199,  2000,  4287,  2041,  1996,  5682,  2275,  5743,  1999,  2930,\n",
       "         1017,  2083,  1996,  9562,  1998, 11453,  1011,  2137,  6960,  4636,\n",
       "         1012,   102,  1998,  2000,  3477,  2005,  1996,  3831, 11727,  1997,\n",
       "         1996,  9562,  1998, 11453,  1011,  2137,  6960,  4636,  1012,   102,\n",
       "         8624,   102])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_tokenized[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokenized['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'sentences'],\n",
       "    num_rows: 6634\n",
       "})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66f56bbc43aa217dde88ff9dc40ab7d7c7e79ce1d561fbdc90d3a27ed8c9354a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('deeplearning-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
